# PLEASE TAKE NOTE: 

# UDFs can only support
# return values of 
# type String.

# Function names that 
# start with __ are
# considered private
# functions and will not
# be directly invokable.

# Matrix Generator for Operations
#
# Generates a matrix using the efficient Coordinate List (COO) format.
# Only non-zero matrix entries are shown, important for sparse matrices.
#   {Row, Column, Value}
#
# Matrix density = num-non-zero-elements / num-total-elements
# The matrixHelper function takes the following parameters:
#   <numRows, numColumns, densityPct, maxValue>
#   1000, 1000, 50, 10 -- 1000x1000 matrix, 50% filled, [-10, 10] values
#   5000, 1000, 10, -5 -- 5000x1000 matrix, 10% filled, [-5, 5] values
#
# Childnodes are equal to files in the directory being ingested. Although this
# generator does not ingest anything, create dummy "part<n>" files to control
# the number of threads.
# For example, this will create 32 threads per node:
#   Using localfile:// or gEnableLocalFiles = true:
#   Node1, Directory /ssd/datasets/matrix: part1, part2, part3,... part32
#   ...
#   Node4, Directory /ssd/datasets/matrix: part1, part2, part3,... part32
# OR create 4 x 32 shared files:
#   Shared directory s3://xcfield/datasets/partdata: part1, part2,..., part128
#
# matrixHelper() uses nodeId and childId to distribute matrix generation
# for rows and cols among the various childnodes. A 100 Million element matrix
# can be generated by a 4-node cluster in a few seconds at full throttle.
#
# MatrixA, MatrixB, MatrixC UDFs generate matrices of varying dimensions.
#
# Matrix Multiply:
#       A x B = A[i,k] * B[k,j] = M[i,j]
#   Xcalar FASJ can accomplish A x B in 3 operations:
#       Join (A.col, B.row) => joinAB: (A.row, B.col, A.val, B.val)
#       Multiply (A.val, B.val) => column prodAB
#       GroupBy (A.row, B.col) Sum (prodAB)
#   Show (AxB)xC = Ax(BxC)
#
# Matrix Transpose: trivial. Switch row and column.
#   Xcalar cannot rename columns. Add new t-row, t-column columns.
#   Use map(add, 0) to populate t-row from column, t-column from row.
#   Show (AxB)T = (B)T x (A)T
#
# Matrix Add/Subtract: trivial.
#
# Matrix Equality:
#   Row Count(A) = Row Count(B) = Row Count(JoinAB(row, col, value))
#
# (Ref: www.simple-talk.com/sql/t-sql-programming/matrix-math-in-sql)
#
import csv
import random
import sys
import xcalar.container.context as ctx

# Set this constant to the number of dummy partfiles
NUM_PARTFILES=64

#
# MatrixA: 10K x 10K x 10% = 10 Million rows (non-zero values)
#
def generateMatrixA(partFile, inStream):
    numRows = 10000
    numCols = 10000
    fillPct = 2
    maxValue = 10

    for row in __matrixHelper(partFile, numRows, numCols, fillPct, maxValue, 0):
        yield row

#
# MatrixB: 10K x 2K x 10% = 2 Million rows (non-zero values)
#
def generateMatrixB(partFile, inStream):
    numRows = 10000
    numCols = 2000
    fillPct = 2
    maxValue = 5

    for row in __matrixHelper(partFile, numRows, numCols, fillPct, maxValue, 0):
        yield row

#
# MatrixTiny: 100 x 100 x 100% = 10,000 rows
#
def generateMatrixTiny(partFile, inStream):
    numRows = 100
    numCols = 100
    fillPct = 100
    maxValue = 35

    for row in __matrixHelper(partFile, numRows, numCols, fillPct, maxValue, 0):
        yield row

# Reads a CSV parameter file to control matrix generation
# Format: <numRows, numCols, fillPct, maxValue>
# Currently using a parameter file will not make it possible to parallelize
# and distribute work because of reliance on the "partfile" trick.
#
def __parfileReader(inStream):
    numRows = 100
    numCols = 100
    fillPct = 100
    maxValue = 10
    seed = 0

    # Read matrix generation parameters (if specified)
    reader = csv.reader(inStream, delimiter=",")
    for row in reader:
        if (len(row) != 5):
            continue
        try:
            input = int(row[0])
            if (input <= 100000):
                numRows = input
            input = int(row[1])
            if (input <= 100000):
                numCols = input
            input = int(row[2])
            if (input <= 100):
                fillPct = input
            input = abs(int(row[3]))
            if (input <= 100):
                maxValue = input
            seed = int(row[4])
        except ValueError:
            continue
        break # only one row in parameter file

    for row in __matrixHelper('part1', numRows, numCols, fillPct, maxValue, 0):
        yield row


# MatrixHelper does all the heavy lifting
# A seed can be passed in to regenerate the same set of matrices.
#
def __matrixHelper(partFile, numRows, numCols, fillPct, maxValue, seed):
    maxValue = abs(maxValue)
    matrixRow = {}
    if (seed != 0):
        random.seed(seed)

    num_nodes = ctx.get_node_count()
    num_threads = NUM_PARTFILES // num_nodes

    # partition with 0-based node and child Id
    childId = ctx.get_xpu_id()
    nodeId = ctx.get_node_id(childId)

#    try:
#        partId = int(''.join(ele for ele in partFile if ele.isdigit()))
#    except ValueError:
#        partId = 0
#        logger.info("Partfile name contains no number, will run serially!")
#
#    if (partId == 0):
#        logger.info("Matrix {} x {}: fillPct {}, maxVal {}, seed {}".
#                     format(numRows, numCols, fillPct, maxValue, seed))

    for row in range(0, numRows):
        # divide up rows between nodes
        if ((row % num_nodes) != nodeId):
            continue
        for col in range(0, numCols):
            # divide up column fields between partFiles
            if ((col % num_threads) != childId):
                continue
            # skip this element based on desired density
            if ((100 * random.random()) > fillPct):
                continue
            matrixRow["Row"] = row + 1 # make rows/cols 1-based
            matrixRow["Column"] = col + 1
            matrixRow["Value"] = random.randint(0-maxValue, maxValue)
            yield matrixRow
